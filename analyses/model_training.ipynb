{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dalex as dx\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from dask_ml.preprocessing import Categorizer\n",
    "from glum import GeneralizedLinearRegressor, GeneralizedLinearRegressorCV\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from core.evaluation.evaluate_predictions import evaluate_predictions\n",
    "from core.visualisation.plot_utils import plot_test_set_predictions\n",
    "from data.sample_split import create_sample_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## GLM model\n",
    "\n",
    " In this section, I will be predicting the daily total national\n",
    "\n",
    " electricity consumptions for UK. Specifically, Ideally,\n",
    "\n",
    " I want a model that can capture the many factors that influence consumptions\n",
    "\n",
    " – daily min and max temperature, wind speed, rain, humidity seasonality, etc.\n",
    "\n",
    "  As a baseline, I will start with a baseline model that only\n",
    "\n",
    " uses a few categorical feature. Then, I will fit a model by introducing\n",
    "\n",
    " the daily weather features. For both models, I will use GLM regressor.\n",
    "\n",
    " I will use a gamma distribution for my model. The target variable, tsd,\n",
    "\n",
    " is a positive real number, which matches the support of the gamma distribution.\n",
    "\n",
    " Second,from the gamma fit I see that statistically tsd has similar shapes to\n",
    "\n",
    " gamma distribution except that tsd exhibits seasonality\n",
    "\n",
    " which has more than 1 peak and valley.\n",
    "\n",
    " Fit daily electricity consumption to gamma distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>is_holiday[0]</th>\n",
       "      <th>day_of_week[0]</th>\n",
       "      <th>day_of_week[1]</th>\n",
       "      <th>day_of_week[2]</th>\n",
       "      <th>day_of_week[3]</th>\n",
       "      <th>day_of_week[4]</th>\n",
       "      <th>day_of_week[5]</th>\n",
       "      <th>day_of_week[6]</th>\n",
       "      <th>week_of_year[1]</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter[3]</th>\n",
       "      <th>quarter[4]</th>\n",
       "      <th>is_weekday[0]</th>\n",
       "      <th>is_weekday[1]</th>\n",
       "      <th>is_weekend[0]</th>\n",
       "      <th>is_weekend[1]</th>\n",
       "      <th>is_summer[0]</th>\n",
       "      <th>is_summer[1]</th>\n",
       "      <th>is_winter[0]</th>\n",
       "      <th>is_winter[1]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coefficient</th>\n",
       "      <td>14.266684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.072191</td>\n",
       "      <td>0.07407</td>\n",
       "      <td>0.071347</td>\n",
       "      <td>0.05033</td>\n",
       "      <td>-0.053149</td>\n",
       "      <td>-0.079263</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00265</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>-0.006176</td>\n",
       "      <td>1.372297e-14</td>\n",
       "      <td>-5.017599e-17</td>\n",
       "      <td>1.368568e-14</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>1.532968e-15</td>\n",
       "      <td>-0.00221</td>\n",
       "      <td>6.826723e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             intercept  is_holiday[0]  day_of_week[0]  day_of_week[1]  \\\n",
       "coefficient  14.266684            0.0        0.049341        0.072191   \n",
       "\n",
       "             day_of_week[2]  day_of_week[3]  day_of_week[4]  day_of_week[5]  \\\n",
       "coefficient         0.07407        0.071347         0.05033       -0.053149   \n",
       "\n",
       "             day_of_week[6]  week_of_year[1]  ...  quarter[3]  quarter[4]  \\\n",
       "coefficient       -0.079263           0.0453  ...    -0.00265    0.001834   \n",
       "\n",
       "             is_weekday[0]  is_weekday[1]  is_weekend[0]  is_weekend[1]  \\\n",
       "coefficient      -0.006176   1.372297e-14  -5.017599e-17   1.368568e-14   \n",
       "\n",
       "             is_summer[0]  is_summer[1]  is_winter[0]  is_winter[1]  \n",
       "coefficient      0.000697  1.532968e-15      -0.00221  6.826723e-16  \n",
       "\n",
       "[1 rows x 86 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GLM baseline model\n",
    "df_model = pd.read_parquet(\n",
    "    Path.cwd().parent / \"data\" / \"model_data\" / \"model_data.parquet\"\n",
    ")\n",
    "df = create_sample_split(df_model, threshold_date=\"2021-01-01\")\n",
    "train = np.where(df[\"sample\"] == \"train\")\n",
    "test = np.where(df[\"sample\"] == \"test\")\n",
    "df_train = df.iloc[train].copy()\n",
    "df_test = df.iloc[test].copy()\n",
    "y = df[\"tsd\"]\n",
    "\n",
    "categorical = [\n",
    "    \"is_holiday\",\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"month\",\n",
    "    \"quarter\",\n",
    "    \"is_weekday\",\n",
    "    \"is_weekend\",\n",
    "    \"is_summer\",\n",
    "    \"is_winter\",\n",
    "]\n",
    "baseline_categorizer = Categorizer(columns=categorical)\n",
    "glm_features = categorical\n",
    "glm_regressor = GeneralizedLinearRegressor(\n",
    "    family=\"gamma\",\n",
    "    scale_predictors=True,\n",
    "    l1_ratio=1,\n",
    "    alphas=1e-1,\n",
    ")\n",
    "\n",
    "X_train_t = baseline_categorizer.fit_transform(df[glm_features].iloc[train])\n",
    "X_test_t = baseline_categorizer.transform(df[glm_features].iloc[test])\n",
    "y_train_t, y_test_t = y.iloc[train], y.iloc[test]\n",
    "\n",
    "glm_regressor.fit(X_train_t, y_train_t)\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"coefficient\": np.concatenate(\n",
    "            ([glm_regressor.intercept_], glm_regressor.coef_)\n",
    "        )\n",
    "    },\n",
    "    index=[\"intercept\"] + glm_regressor.feature_names_,\n",
    ").T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"baseline_glm\"] = glm_regressor.predict(X_test_t)\n",
    "df_train[\"baseline_glm\"] = glm_regressor.predict(X_train_t)\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "evaluate_predictions(\n",
    "    df_test,\n",
    "    outcome_column=\"tsd\",\n",
    "    tweedie_power=2.0,\n",
    "    preds_column=\"baseline_glm\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(\n",
    "    df_train,\n",
    "    outcome_column=\"tsd\",\n",
    "    tweedie_power=2.0,\n",
    "    preds_column=\"baseline_glm\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_set_predictions(\n",
    "    df_test, \"baseline_glm\", \"GLM baseline model prediction on test set\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM model with weather numeric data\n",
    "numeric = [\n",
    "    \"min_temp °c\",\n",
    "    \"max_temp °c\",\n",
    "    \"rain mm\",\n",
    "    \"humidity %\",\n",
    "    \"cloud_cover %\",\n",
    "    \"wind_speed km/h\",\n",
    "]\n",
    "\n",
    "# Let's put together pipeline for the GLM model\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"numeric\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"scale\", StandardScaler()),\n",
    "                ]\n",
    "            ),\n",
    "            numeric,\n",
    "        ),\n",
    "        (\"cat\", OneHotEncoder(sparse_output=False, drop=\"first\"), categorical),\n",
    "    ]\n",
    ")\n",
    "preprocessor.set_output(transform=\"pandas\")\n",
    "model_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\n",
    "            \"estimate\",\n",
    "            glm_regressor,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# let's have a look at the pipeline\n",
    "model_pipeline\n",
    "\n",
    "# let's check that the transforms worked\n",
    "model_pipeline[:-1].fit_transform(df_train)\n",
    "\n",
    "model_pipeline.fit(df_train, y_train_t)\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"coefficient\": np.concatenate(\n",
    "            ([model_pipeline[-1].intercept_], model_pipeline[-1].coef_)\n",
    "        )\n",
    "    },\n",
    "    index=[\"intercept\"] + model_pipeline[-1].feature_names_,\n",
    ").T\n",
    "\n",
    "df_test[\"demand_glm2\"] = model_pipeline.predict(df_test)\n",
    "df_train[\"demand_glm2\"] = model_pipeline.predict(df_train)\n",
    "\n",
    "evaluate_predictions(\n",
    "    df_test,\n",
    "    outcome_column=\"tsd\",\n",
    "    tweedie_power=2.0,\n",
    "    preds_column=\"demand_glm2\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_set_predictions(\n",
    "    df_test,\n",
    "    \"demand_glm2\",\n",
    "    \"GLM model with weather data prediction on test set\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = pd.Timestamp(\"2022-02-01\")\n",
    "end = pd.Timestamp(\"2022-06-01\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.plot(\n",
    "    df_test.loc[(df_test.index > begin) & (df_test.index < end)].index,\n",
    "    df_test.loc[(df_test.index > begin) & (df_test.index < end)][\"tsd\"],\n",
    "    \"o\",\n",
    "    label=\"Test set\",\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    df_test.loc[(df_test.index > begin) & (df_test.index < end)].index,\n",
    "    df_test.loc[(df_test.index > begin) & (df_test.index < end)][\n",
    "        \"demand_glm2\"\n",
    "    ],\n",
    "    \"o\",\n",
    "    label=\"Prediction\",\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"center\", bbox_to_anchor=(1.075, 0.5))\n",
    "\n",
    "ax.set_title(\"Prediction on test set - Two weeks\")\n",
    "ax.set_ylabel(\"Energy Demand (MW)\")\n",
    "ax.set_xlabel(\"Date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM Cross-validation model for hyperparameter tuning\n",
    "all_features = categorical + numeric\n",
    "X_train_t = baseline_categorizer.fit_transform(df[all_features].iloc[train])\n",
    "X_test_t = baseline_categorizer.transform(df[all_features].iloc[test])\n",
    "y_train_t, y_test_t = y.iloc[train], y.iloc[test]\n",
    "\n",
    "glmcv = GeneralizedLinearRegressorCV(\n",
    "    family=\"gamma\",\n",
    "    alphas=None,  # default\n",
    "    min_alpha=None,  # default\n",
    "    min_alpha_ratio=None,  # default\n",
    "    l1_ratio=[0, 0.1, 0.3, 0.5, 0.7, 0.8, 1.0],\n",
    "    fit_intercept=True,\n",
    "    max_iter=150,\n",
    ")\n",
    "glmcv.fit(X_train_t, y_train_t)\n",
    "print(f\"Chosen alpha:    {glmcv.alpha_}\")\n",
    "print(f\"Chosen l1 ratio: {glmcv.l1_ratio_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"demand_cv_glm\"] = glmcv.predict(X_test_t)\n",
    "df_train[\"demand_cv_glm\"] = glmcv.predict(X_train_t)\n",
    "evaluate_predictions(\n",
    "    df_test,\n",
    "    outcome_column=\"tsd\",\n",
    "    tweedie_power=2.0,\n",
    "    preds_column=\"demand_cv_glm\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_set_predictions(\n",
    "    df_test,\n",
    "    \"demand_cv_glm\",\n",
    "    \"GLM model with weather data prediction on test set\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## LGBM model\n",
    "\n",
    " The second forecasting method I will use is LGBM.\n",
    "\n",
    " The first LGBM model is a simple model for which some of the parameters\n",
    "\n",
    " are defined and the data is split into train and test sets.\n",
    "\n",
    " This model is fairly simple, but it's a great baseline.\n",
    "\n",
    " The second model is a tuned LGBM model. The parameters are tuned using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LGBM model\n",
    "model_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"estimate\",\n",
    "            LGBMRegressor(\n",
    "                objective=\"gamma\",\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.01,\n",
    "                num_leaves=6,\n",
    "                max_depth=3,\n",
    "                random_state=43,\n",
    "                early_stopping_rounds=50,\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_pipeline.fit(\n",
    "    X_train_t,\n",
    "    y_train_t,\n",
    "    estimate__eval_set=[(X_test_t, y_test_t), (X_train_t, y_train_t)],\n",
    ")\n",
    "lgb.plot_metric(model_pipeline[0])\n",
    "\n",
    "df_test[\"demand_lgbm\"] = model_pipeline.predict(X_test_t)\n",
    "df_train[\"demand_lgbm\"] = model_pipeline.predict(X_train_t)\n",
    "evaluate_predictions(\n",
    "    df_test,\n",
    "    outcome_column=\"tsd\",\n",
    "    tweedie_power=2.0,\n",
    "    preds_column=\"demand_lgbm\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_set_predictions(\n",
    "    df_test, \"demand_lgbm\", \"Simple LGBM predictions on test data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature importances\n",
    "feature_importances = model_pipeline.named_steps[\n",
    "    \"estimate\"\n",
    "].feature_importances_\n",
    "\n",
    "# Get the feature names from the preprocessor\n",
    "feature_names = (\n",
    "    model_pipeline.named_steps[\"estimate\"].booster_.feature_name(),\n",
    ")\n",
    "\n",
    "# # Create a DataFrame to display the feature importances\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    index=feature_names[0],\n",
    "    data=feature_importances.tolist(),\n",
    "    columns=[\"Importance\"],\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "\n",
    "feature_importance_df.plot(kind=\"barh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune the pipeline to reduce overfitting\n",
    "\n",
    "# Note: Typically we tune many more parameters and larger grids,\n",
    "# but to save compute time here, we focus on getting the learning rate\n",
    "# and the number of estimators somewhat aligned\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    \"estimate__learning_rate\": [0.01, 0.02, 0.03, 0.04, 0.05, 0.1],\n",
    "    \"estimate__n_estimators\": [1000],\n",
    "    \"estimate__num_leaves\": [6, 12, 24],\n",
    "    \"estimate__min_child_weight\": [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with k-fold cross-validation\n",
    "model_pipeline.named_steps[\"estimate\"].set_params(early_stopping_rounds=25)\n",
    "cv = GridSearchCV(\n",
    "    model_pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "cv.fit(\n",
    "    X_train_t,\n",
    "    y_train_t,\n",
    "    estimate__eval_set=[(X_test_t, y_test_t), (X_train_t, y_train_t)],\n",
    ")\n",
    "\n",
    "lgbm_tuning = cv.best_estimator_\n",
    "df_test[\"demand_tuning_lgbm\"] = cv.best_estimator_.predict(X_test_t)\n",
    "df_train[\"demand_tuning_lgbm\"] = cv.best_estimator_.predict(X_train_t)\n",
    "\n",
    "evaluate_predictions(\n",
    "    df_test,\n",
    "    outcome_column=\"tsd\",\n",
    "    tweedie_power=2.0,\n",
    "    preds_column=\"demand_tuning_lgbm\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "lgbm_tuning.fit(\n",
    "    X_train_t,\n",
    "    y_train_t,\n",
    "    estimate__eval_set=[(X_test_t, y_test_t), (X_train_t, y_train_t)],\n",
    ")\n",
    "\n",
    "lgb.plot_metric(lgbm_tuning[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_set_predictions(\n",
    "    df_test, \"demand_tuning_lgbm\", \"Tunned LGBM predictions on test data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 5 most important features\n",
    "top_5_features = feature_importance_df.head(5).index.tolist()\n",
    "top_5 = []\n",
    "for i in top_5_features:\n",
    "    if \"_%\" in i:\n",
    "        i = i.replace(\"_%\", \" %\")\n",
    "    if \"_°c\" in i:\n",
    "        i = i.replace(\"_°c\", \" °c\")\n",
    "    top_5.append(i)\n",
    "\n",
    "# Create an explainer for the LGBM model\n",
    "explainer_lgbm = dx.Explainer(\n",
    "    lgbm_tuning, X_test_t, y_test_t, label=\"Tunned LGBM Model\"\n",
    ")\n",
    "\n",
    "# Plot partial dependence plots for the top 5 features\n",
    "pdp = explainer_lgbm.model_profile(variables=top_5, type=\"partial\")\n",
    "pdp.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap = explainer_lgbm.predict_parts(X_test_t.head(1), type=\"shap\")\n",
    "\n",
    "shap.plot()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
